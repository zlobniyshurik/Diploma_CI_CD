# Диплом


## Этап 1 *(Регистрация доменного имени)*

- В **[nic.ru](https://nic.ru)** специально для диплома был зарегистрирован домен **[diplomshurika.ru](http://diplomshurika.ru)**

- В качестве бесплатного публичного DNS воспользовался сервисом от **[dns.he.net](https://dns.he.net)**  
  *из бонусов:* 
  - *возможно динамическое управление зонами с помощью GET/POST-запросов*
  - *существует плагин с поддержкой ***[dns.he.net](https://dns.he.net)*** для скрипта ***[acme.sh](https://github.com/acmesh-official/acme.sh)***, обеспечивающего автоматизированное обновление сертификатов от* ***[Let's Encrypt](https://letsencrypt.org/ru/)***

---
## Этап 2 *(Создание инфраструктуры)*
На данном этапе встретились первые подводные камни:

Жёстко прописывать в скрипты все учётки/явки/пароли, мягко говоря, недальновидно. Однако и выделить все настройки в отдельный файл с переменными не получилось - движок `Terraform`'а почему-то не понимает переменные в секции с бэкендом *(баги по этому поводу открыты уже более 5 лет)*.  
Пришлось настройки бэкенда выносить в отдельный файл `backend.conf` формата "ключ=значение", а инициализацию `Terraform`'а производить с дополнительными параметрами:
```
terraform init -backend-config=backend.conf
```
*Пример файла* ***backend.conf*** *:*
```
endpoint   = "storage.yandexcloud.net"
bucket     = "<имя бакета>"
region     = "ru-central1"
key        = "<путь к файлу состояния в бакете>/<имя файла состояния>.tfstate"
access_key = "<идентификатор статического ключа>"
secret_key = "<секретный ключ>"

skip_region_validation      = true
skip_credentials_validation = true
```
*Файл с остальными ключами к ЯндексОблаку ***private.tf*** на данный момент выглядит примерно так:*
```terraform
########################################
# General settings for Yandex provider #
########################################

variable "ya_token" {
  description = "Yandex token"
  type        = string
  default     = "<Яндекс токен>"
  sensitive   = true
}

variable "ya_cloud_id" {
  description = "Yandex cloud_id"
  type        = string
  default     = "<Яндекс cloud_id>"
  sensitive   = true
}

variable "ya_folder_id" {
  description = "Yandex folder_id"
  type        = string
  default     = "<Яндекс folder_id>"
  sensitive   = true
}

variable "ya_zone" {
  description = "Yandex geozone"
  type        = string
  default     = "ru-central1-a"
}
```

В результате `Terraform` работает и сохраняет стейты в YandexCloud:  
![Стейты в S3-хранилище](./pic/diplom_2_1.png)

---
## Этап 3 *(Установка Nginx и LetsEncrypt)*

1. Установлен самописный скрипт, который автоматически прописывает в A-запись доменной зоны текущий IP-адрес нашего реверс-прокси после каждого запуска *(руками вбивать это каждый раз - откровенно лениво, а покупать "белый" IP нам жадность не позволяет)*.

   Сам скрипт *(фрагмент из ***proxy.tf***)*  
   ```bash
   #!/bin/bash
   curl -4 "https://"${var.my_domain_tld}":"${var.he_net_key}"@dyn.dns.he.net/nic/update?hostname="${var.my_domain_tld}
   ```
   Соответственно, сделан простенький сервис, который и запускает вышеозначенный скрипт после каждого запуска/перезагрузки виртуальной машины в облаке.
   Спасибо **[dns.he.net](https://dns.he.net)**, где предусмотрели такой несложный способ работы с динамическими IP.

---
2. Для автоматического получения и обновления сертификатов от **[Let's Encrypt](https://letsencrypt.org/)** воспользовался скриптом **[acme.sh](https://github.com/acmesh-official/acme.sh)**.

   Из плюсов данного решения:
- Не тянет за собой кучу зависимостей
- Работает без root-прав
- Автоматом по расписанию пытается обновить сертификаты.
- *(Самое главное!)* Умеет работать с **[dns.he.net](https://dns.he.net)**

***Важное примечание:*** *Не будем мелочиться и сразу же получим так называемый wildcard-сертификат. То есть один сертификат на все возможные поддомены третьего уровня для нашего домена.*

---
3. Так как впоследствии нам придётся подымать кластер из MySQL-нод, то потребуется и какой-нибудь балансировщик, через который мы будем до этих нод стучаться.  
Задействуем **nginx**, скормив ему примерно такой конфиг 
*(автоматом генерится ***Terraform***'ом по числу доступных нод)*
```
stream {
        upstream stream_backend {
            least_conn;

            server db01.example.com:3306 max_fails=2 fail_timeout=30s;

            server db02.example.com:3306 max_fails=2 fail_timeout=30s;

          }

        server {
            listen 3306;
            proxy_connect_timeout 1s;
            proxy_timeout 3s;
            proxy_pass stream_backend;
          }
}
```

---
## Этап 3½ *(JumpHost и прокси)*

1. Поскольку с публичными адресами для всех виртуальных машин кроме реверс-прокси у нас проблемы, то приходится использовать нашу реверс-прокси ноду ещё и как JumpHost для трансляции ssh-запросов в глубины сети с "серыми" IP.

В принципе, ничего экстраординарного, но вот проверку SSH-ключей от новых хостов внутри реверс-прокси пришлось отключить, иначе **Ansible** подвисал при установлении соединения с нодами.

Для отключения проверки ключей воспользовался **Terraform**'ом, поправив на реверс-прокси ноде файлик ***`~/.ssh/config`***,
вписав туда
```
Host *
    StrictHostKeyChecking no
```

---
2. Ещё одним подводным камнем оказалось отсутствие интернета на тех нодах, где нет публичных IP. Соответственно, с установкой софта возникли проблемы.

В качестве решения на реверс-прокси ноду был установлен прокси **Squid** *(минимум настроек и достаточное количество возможностей)*.

На нодах, оставшихся без публичных IP, оказалось достаточным прописать прокси в настройках пакетного менеджера **dnf**. После чего возможность установки ПО из внешних репозиториев восстановилась.

По идее, можно было бы ещё и прописать ACLы с паролями для доступа к прокси для увеличения безопасности, но оставим это потомкам.

---
## Этап 4 *(Установка кластера MySQL)*

К сожалению, полученный результат назвать ***отказоустойчивым*** кластером рука не подымается:

Да, у нас целых две ноды. Да, при желании можно использовать балансер на **nginx**'е из этапа 3...

Но при репликации **Master-Slave** кластер будет отказоустойчивым ***только при чтении*** данных из базы. В этом случае всё красиво - данные берутся через балансер из любой живой ноды.

Но как только требуется что-то записать - записывать можно исключительно в master-ноду, а она и отключиться может. И тут наш примитивный вариант балансера только помешает - им нельзя пользоваться при записи, дабы не подключиться случайно к slave-ноде.

А вот если бы была бы репликация **Master-Master**, то у нас действительно получился бы достаточно отказоустойчивый кластер, выживающий при отключении любой из нод. В этом случае писать/читать можно было бы на любую живую ноду, а оставшиеся подтянули бы к себе изменённые данные после восстановления жизнеспособности.

Но, в конце-концов, оно таки работает. Базы/юзеры создаются, данные успешно реплицируются.

---
## Этап 5 *(Установка WordPress)*


